{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymongo\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from math import log\n",
    "\n",
    "db_court=pymongo.MongoClient(host='10.244.2.3', port=27017,connect=False).test.robot\n",
    "md5 = lambda s: hashlib.md5(s).hexdigest()\n",
    "def texts():\n",
    "    texts_set = set()\n",
    "    for court_case in tqdm(db_court.find(no_cursor_timeout=True).limit(1230080)):\n",
    "        if md5(court_case['question'].encode('utf-8')) in texts_set:\n",
    "            pass\n",
    "        else:\n",
    "            texts_set.add(md5(court_case['question'].encode('utf-8')))\n",
    "        for t in re.split('[^\\u4e00-\\u9fa50-9a-zA-Z]+', court_case['question']):\n",
    "            if t:\n",
    "                yield t\n",
    "    print('最终计算了%s篇文章'%len(texts_set))\n",
    "    return texts_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1230080it [18:31, 1107.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终计算了1145812篇文章\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "n = 4\n",
    "min_count = 1\n",
    "ngrams = defaultdict(int)\n",
    "\n",
    "for t in texts():\n",
    "    for i in range(len(t)):\n",
    "        for j in range(1, n+1):\n",
    "            if i+j <= len(t):\n",
    "                ngrams[t[i:i+j]] += 1\n",
    "\n",
    "ngrams = {i:j for i,j in ngrams.items() if j >= min_count}\n",
    "total = 1.*sum([j for i,j in ngrams.items() if len(i) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29690397"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20455"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec,FastText\n",
    "import gensim\n",
    "fasttest_vec = FastText.load('/opt/gongxf/python3_pj/Robot/2_fasttext2vec/all_session/fasttext_cbow.model')\n",
    "len(fasttest_vec.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#凝固度 刷选\n",
    "min_proba = {2:50, 3:50, 4:50}\n",
    "# min_proba={2:0, 3:1, 4:1}\n",
    "solidify_ngrams=dict()\n",
    "def is_keep(s):\n",
    "    if len(s)>2:\n",
    "        score = min([total*ngrams[s]/(ngrams[s[:i+1]]*ngrams[s[i+1:]]) for i in range(len(s)-1)])\n",
    "    else:\n",
    "        score=total/ngrams[s]\n",
    "    return score\n",
    "\n",
    "for word in fasttest_vec.wv.vocab.keys():\n",
    "    if word in ngrams:\n",
    "        proba=is_keep(word)\n",
    "        fre=ngrams[word]\n",
    "        solidify_ngrams[word]=[fre,proba]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19675"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(solidify_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist \n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_entropy=0.8\n",
    "# def entropy(alist):\n",
    "#     f=FreqDist(alist)\n",
    "#     ent=(-1)*sum([i/len(alist)*math.log(i/len(alist)) for i in f.values()])\n",
    "#     return ent\n",
    "# #筛选左右熵\n",
    "# lr_word=dict()\n",
    "# entory_ngrams_=[]\n",
    "# for i in solidify_ngrams.keys():\n",
    "#     lr_word[i]=[]\n",
    "\n",
    "# for text in texts():\n",
    "#     for i in solidify_ngrams.keys():\n",
    "#         lr1=re.findall('(.)%s(.)'%i,text)\n",
    "#         if lr1:\n",
    "#             lr_word[i].extend(lr1)\n",
    "\n",
    "# for i in solidify_ngrams.keys():\n",
    "#     left_entropy=entropy([w[0] for w in lr_word[i]])\n",
    "#     right_entropy=entropy([w[1] for w in lr_word[i]])\n",
    "#     solidify_ngrams[i].append(left_entropy)\n",
    "#     solidify_ngrams[i].append(right_entropy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_entropy = 0.8\n",
    "from multiprocessing import Process, cpu_count, Manager\n",
    "manager = Manager()\n",
    "\n",
    "def entropy(alist):\n",
    "    f = FreqDist(alist)\n",
    "    ent = (-1) * sum([i / len(alist) * math.log(i / len(alist)) for i in f.values()])\n",
    "    return ent\n",
    "\n",
    "\n",
    "# 筛选左右熵\n",
    "lr_word = manager.dict()\n",
    "entory_ngrams_ = []\n",
    "for i in solidify_ngrams.keys():\n",
    "    lr_word[i] = []\n",
    "\n",
    "corpus = list(solidify_ngrams.keys())\n",
    "cate_question_len = len(corpus)  # 该类别长度\n",
    "multi_num = cpu_count()\n",
    "batch_size = int(cate_question_len / multi_num)\n",
    "corpus_multi = []\n",
    "for i in range(multi_num - 1):\n",
    "    corpus_multi.append(corpus[i * batch_size:(i + 1) * batch_size])\n",
    "corpus_multi.append(corpus[(multi_num - 1) * batch_size:])  # 每个进程的数据集\n",
    "\n",
    "\n",
    "def fun(corpus_multi):\n",
    "    for text in texts():\n",
    "        for i in corpus_multi:\n",
    "            lr1 = re.findall('(.)%s(.)' % i, text)\n",
    "            if lr1:\n",
    "                lr_word[i].extend(lr1)\n",
    "    return lr_word\n",
    "\n",
    "\n",
    "proc_record = []\n",
    "manager = Manager()\n",
    "for i in range(multi_num):\n",
    "    p = Process(target=fun, args=(corpus_multi[i],))\n",
    "    p.start()\n",
    "    proc_record.append(p)\n",
    "for p in proc_record:\n",
    "    p.join()\n",
    "\n",
    "for i in solidify_ngrams.keys():\n",
    "    left_entropy = entropy([w[0] for w in lr_word[i]])\n",
    "    right_entropy = entropy([w[1] for w in lr_word[i]])\n",
    "    solidify_ngrams[i].append(left_entropy)\n",
    "    solidify_ngrams[i].append(right_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
